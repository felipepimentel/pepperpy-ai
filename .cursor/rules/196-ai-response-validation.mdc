---
title: AI Response Validation and Hallucination Prevention
description: USE WHEN validating AI-generated code to prevent hallucinations, ensure accuracy, and maintain consistency with project standards
globs:
  - "**/*.py"
priority: 195
alwaysApply: true
---

# AI Response Validation and Hallucination Prevention

## Overview

This rule provides essential strategies to prevent AI hallucinations (fabricated or incorrect responses) and ensure AI-generated code adheres to project standards. Following these guidelines will dramatically reduce errors, inconsistencies, and inaccuracies in AI-assisted development.

## Before Generating ANY Code

Always follow these steps before asking for code generation:

1. **Check file existence first**:
   ```bash
   # Verify if similar files exist
   find pepperpy -name "*similar_name*.py"
   ```

2. **Understand module structure**:
   ```bash
   # Check structure of the relevant module
   ls -la pepperpy/target_module/
   ```

3. **Find similar implementations**:
   ```bash
   # Look for similar implementations
   grep -r "class SimilarClass" pepperpy/
   ```

## Validate ALL Generated Code

After receiving AI-generated code, verify:

### 1. Import Validation

✅ **All imports must exist in the codebase**

Check every import statement and ensure it references real modules:

```python
# CORRECT: Imports reference existing modules
from pepperpy.core.base import Provider
from pepperpy.llm.base import LLMProvider

# WRONG: Referencing non-existent modules
from pepperpy.nonexistent import Something  # ERROR!
```

### 2. Class/Function Reference Validation

✅ **All referenced classes/functions must exist**

Ensure all references are to actual code:

```python
# CORRECT: Using existing class
provider = OpenAIProvider(config)

# WRONG: Using non-existent class
provider = GPT5Provider(config)  # ERROR! This doesn't exist
```

### 3. File Location Validation

✅ **Files must be created in correct locations**

Follow the established patterns:

```
pepperpy/module_name/        # Module root
├── __init__.py              # Public exports
├── base.py                  # Core interfaces
├── providers/               # ONLY provider implementations here
│   ├── __init__.py
│   └── specific.py
└── utils/                   # Module-specific utilities
```

### 4. Pattern Validation

✅ **Implementation must follow established patterns**

Look for similar files and follow their patterns:

```python
# Example pattern from other modules
class SomeProvider(BaseProvider):
    def __init__(self, config: Config):
        self.config = config
        
    async def initialize(self) -> None:
        # Initialize resources
        
    async def cleanup(self) -> None:
        # Clean up resources
```

## Common Hallucination Red Flags

Watch for these warning signs of AI hallucinations:

1. **Made-up module names**:
   ```python
   from pepperpy.ai.models import AIModel  # No such module!
   ```

2. **Non-existent base classes**:
   ```python
   class MyProvider(ModelProvider):  # ModelProvider doesn't exist!
   ```

3. **Invented API methods**:
   ```python
   result = client.get_embedding_sync(text)  # No such method!
   ```

4. **Inconsistent parameters**:
   ```python
   # Method signature: def process(self, text: str, options: dict)
   processor.process(text, flags=True)  # Wrong parameter name!
   ```

5. **Import confusion**:
   ```python
   from pepperpy.llm import LLMProvider
   from pepperpy.llm.providers import OpenAIProvider
   
   # But then uses:
   provider = openai.OpenAIProvider()  # Wrong namespace!
   ```

## Hallucination Prevention Strategies

### 1. Pre-Generation Verification

Before generating complex code:

```bash
# Establish facts first
python scripts/refactor.py extract-api --module pepperpy/target_module
```

### 2. Incremental Validation

Validate code in small increments:

```bash
# Generate and check interface first
# Then generate and check implementation
# Then generate and check usage example
```

### 3. Pattern Matching

Find and follow existing patterns:

```bash
# Find similar patterns
grep -r "class \w*Provider" pepperpy/
```

### 4. Code Structure Validation

Check generated code structure:

```python
# CORRECT pattern:
class MyClass:
    def __init__(self, config):
        self.config = config
    
    async def method(self):
        # Implementation
```

## Real Examples of Hallucinations

❌ **Provider Invention**
```python
# HALLUCINATION: Creating fake provider
from pepperpy.llm.providers import MistralProvider  # Doesn't exist yet!
```

❌ **Method Invention**
```python
# HALLUCINATION: Inventing methods
document.embed_with_model("text-embedding-3")  # No such method!
```

❌ **Configuration Invention**
```python
# HALLUCINATION: Made-up configuration format
config = {
    "model": "gpt-4",
    "provider_options": {  # Not how our config works!
        "temperature": 0.7
    }
}
```

## MUST CHECK Before Implementing

1. ✓ Module structure matches project patterns
2. ✓ All imports reference real modules
3. ✓ All classes and functions exist
4. ✓ Parameter names match existing patterns
5. ✓ Configuration follows project standards
6. ✓ Error handling matches project patterns

## When Modifying Existing Code

1. ✓ Understand exact functionality first
2. ✓ Maintain parameter compatibility
3. ✓ Preserve error handling
4. ✓ Follow existing patterns exactly
5. ✓ Keep same import structure
6. ✓ Maintain type annotations

## Progressive Implementation Approach

Instead of generating large code blocks at once:

1. **Start with skeleton structure**
2. **Implement and validate one component** at a time
3. **Verify each component** before proceeding
4. **Integrate with existing code** incrementally

Example workflow:
```bash
# First, generate and validate the interface
python scripts/refactor.py gen-class --name MyInterface --output path/to/interface.py --desc "Description"

# Then implement and validate the concrete implementation
python scripts/refactor.py gen-class --name MyImplementation --output path/to/implementation.py
```

## Preventing Framework Inconsistencies

### 1. Provide Explicit Context

When requesting AI assistance:

* **Share relevant file sections** to establish patterns
* **Specify exact import paths** to use
* **Reference similar implementations** as examples

### 2. Use Template-Based Generation

Always use the project's code generation tools instead of free-form generation:

```bash
# Generate new module with proper structure
python scripts/refactor.py gen-module --output pepperpy/new_module.py --desc "Description"

# Generate new provider implementation with proper structure
python scripts/refactor.py gen-provider --name NewProvider --output path/to/provider.py --desc "Description"
```

### 3. Verify Framework Integration

Test AI-generated code integration with the framework:

```bash
# Run specific tests for the generated component
python -m pytest tests/path/to/relevant_test.py -v

# Check for import errors
python -c "import pepperpy.path.to.new_module"
```

## Verification Automation

Create automated verification for AI-generated code:

```bash
# Create a verification script for AI code
python scripts/refactor.py rule-management --verify --rule 195 --test-case "new_component"
```

## Module Structure Validation

Before generating or modifying code, validate the module structure:

### 1. File Location Checklist

✓ Core interfaces and components go in `base.py`:
```python
# base.py - CORRECT
class MyComponent(Component):
    pass

# component.py - WRONG
class MyComponent(Component):  # Should be in base.py
    pass
```

✓ Provider implementations go in `providers/`:
```python
# providers/azure.py - CORRECT
class AzureProvider(MyProvider):
    pass

# azure.py - WRONG
class AzureProvider(MyProvider):  # Should be in providers/azure.py
    pass
```

### 2. Structure Validation Script

Use the validation script before committing:

```bash
python scripts/refactor.py validate --directory pepperpy

# Example output:
# ERROR: component.py found, should be in base.py
# ERROR: azure.py found outside providers/
```

### 3. Common Structure Mistakes

1. **Separate Component Files**:
   ```python
   # WRONG: Creating component.py
   from .component import MyComponent  # Should be in base.py
   
   # CORRECT: Everything in base.py
   from .base import MyComponent
   ```

2. **Provider Files in Root**:
   ```python
   # WRONG: Creating azure.py in root
   from .azure import AzureProvider  # Should be in providers/
   
   # CORRECT: Using providers/azure.py
   from .providers.azure import AzureProvider
   ```

3. **Mixed Responsibilities**:
   ```python
   # WRONG: Mixing provider implementation with interface
   class Provider(Protocol):  # Should be in base.py
       pass
       
   class AzureProvider(Provider):  # Should be in providers/azure.py
       pass
   ```

## Conclusion

By following these AI response validation guidelines, you can prevent hallucinations, ensure accuracy, and maintain consistency in AI-generated code. Always verify AI outputs against actual project structures and use the project's refactoring tools to validate and integrate new code properly. 