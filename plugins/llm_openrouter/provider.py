"""OpenRouter provider for LLM tasks.

This module provides the OpenRouter provider implementation for LLM tasks,
supporting various models through OpenRouter's API.
"""

import json
from collections.abc import AsyncIterator
from typing import Any, Dict, List, Optional, TypeVar, Union

import httpx

from pepperpy.llm import (
    GenerationChunk,
    GenerationResult,
    LLMProvider,
    Message,
    MessageRole,
)
from pepperpy.plugin import ProviderPlugin

T = TypeVar("T", bound="OpenRouterProvider")


class OpenRouterProvider(LLMProvider, ProviderPlugin):
    """OpenRouter provider for LLM tasks."""

    # These attributes will be auto-bound from plugin.yaml with fallback defaults
    api_key: str = ""  # Will be filled by framework from env vars
    model: str = "openai/gpt-3.5-turbo"
    base_url: str = "https://openrouter.ai/api/v1"
    temperature: float = 0.7
    max_tokens: int = 1024
    client: Optional[httpx.AsyncClient] = None

    def __init__(self, **kwargs: Any) -> None:
        """Initialize the OpenRouter provider."""
        super().__init__(**kwargs)
        self.client = None

        # Ensure we set the API key if provided in kwargs
        if "api_key" in kwargs:
            self.api_key = kwargs["api_key"]

    @classmethod
    def from_config(cls, **config) -> "OpenRouterProvider":
        """Create a provider from configuration."""
        instance = cls()

        # Base URL
        if "base_url" in config:
            instance.base_url = config["base_url"]

        # Model
        if "model" in config:
            instance.model = config["model"]

        # Temperature
        if "temperature" in config:
            instance.temperature = float(config["temperature"])

        # Max tokens
        if "max_tokens" in config:
            instance.max_tokens = int(config["max_tokens"])

        # API key - check both the original api_key and our provider-prefixed key
        if "api_key" in config:
            instance.api_key = config["api_key"]
        elif "openrouter_api_key" in config:
            instance.api_key = config["openrouter_api_key"]

        return instance

    async def initialize(self) -> None:
        """Initialize the OpenRouter client."""

        self.client = httpx.AsyncClient(
            base_url=self.base_url,
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "HTTP-Referer": "https://github.com/pimentel/pepperpy",
                "X-Title": "PepperPy Framework",
            },
        )

    def _convert_messages(
        self, messages: Union[str, List[Message]]
    ) -> List[Dict[str, Any]]:
        """Convert PepperPy messages to OpenRouter format."""
        # Handle single string as user message
        if isinstance(messages, str):
            return [{"role": "user", "content": messages}]

        # Convert list of Message objects
        openrouter_messages = []
        for msg in messages:
            message_dict = {
                "role": msg.role.value,
                "content": msg.content,
            }
            if msg.name:
                message_dict["name"] = msg.name

            openrouter_messages.append(message_dict)

        return openrouter_messages

    async def generate(
        self,
        messages: Union[str, List[Message]],
        **kwargs: Any,
    ) -> GenerationResult:
        """Generate text using the OpenRouter API."""
        # Convert messages to OpenRouter format
        openrouter_messages = self._convert_messages(messages)

        # Use auto-bound attributes with kwargs overrides
        temperature = kwargs.get("temperature", self.temperature)
        max_tokens = kwargs.get("max_tokens", self.max_tokens)
        model = kwargs.get("model", self.model)

        # Make API call
        if not self.client:
            await self.initialize()
            assert self.client is not None

        response = await self.client.post(
            "/chat/completions",
            json={
                "model": model,
                "messages": openrouter_messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
            },
        )

        # Handle errors
        response.raise_for_status()
        completion = response.json()

        if not completion.get("choices"):
            raise ValueError("No content generated by OpenRouter")

        content = completion["choices"][0]["message"]["content"] or ""

        # Build result
        if isinstance(messages, str):
            messages_list = [Message(role=MessageRole.USER, content=messages)]
        else:
            messages_list = messages.copy()

        # Add response to messages
        messages_list.append(Message(role=MessageRole.ASSISTANT, content=content))

        # Extract usage statistics
        usage = completion.get("usage")

        return GenerationResult(
            content=content,
            messages=messages_list,
            usage=usage,
            metadata={
                "model": completion.get("model"),
                "created": completion.get("created"),
                "id": completion.get("id"),
            },
        )

    async def generate_stream(
        self,
        messages: Union[str, List[Message]],
        **kwargs: Any,
    ) -> AsyncIterator[GenerationChunk]:
        """Generate text in streaming mode using the OpenRouter API."""
        # Convert messages to OpenRouter format
        openrouter_messages = self._convert_messages(messages)

        # Use auto-bound attributes with kwargs overrides
        temperature = kwargs.get("temperature", self.temperature)
        max_tokens = kwargs.get("max_tokens", self.max_tokens)
        model = kwargs.get("model", self.model)

        # Make API call with streaming
        if not self.client:
            await self.initialize()
            assert self.client is not None

        response = await self.client.post(
            "/chat/completions",
            json={
                "model": model,
                "messages": openrouter_messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": True,
            },
            timeout=60.0,  # Extended timeout for streaming
        )

        # Handle errors
        response.raise_for_status()

        # Process the streaming response
        async for line in response.aiter_lines():
            line = line.strip()

            # Skip empty lines or non-data lines
            if not line or line == "data: [DONE]":
                continue

            if line.startswith("data: "):
                data = line[6:]  # Remove "data: " prefix
                try:
                    chunk = json.loads(data)

                    if not chunk.get("choices"):
                        continue

                    delta = chunk["choices"][0].get("delta", {})
                    content = delta.get("content", "")

                    if content:
                        yield GenerationChunk(
                            content=content,
                            finish_reason=chunk["choices"][0].get("finish_reason"),
                            metadata={"model": model},
                        )
                except Exception:
                    # Skip invalid chunks
                    continue

    async def stream(
        self,
        messages: Union[str, List[Message]],
        **kwargs: Any,
    ) -> AsyncIterator[GenerationChunk]:
        """Generate text in a streaming fashion (alias for generate_stream).

        Args:
            messages: String prompt or list of messages
            **kwargs: Additional generation options

        Returns:
            AsyncIterator yielding GenerationChunk objects

        Raises:
            RuntimeError: If generation fails
        """
        async for chunk in self.generate_stream(messages, **kwargs):
            yield chunk

    async def cleanup(self) -> None:
        """Clean up provider resources."""
        if self.client:
            await self.client.aclose()
            self.client = None
