name: ai_gateway_mock
version: 0.1.0
description: Mock provider for AI Gateway testing and development
author: PepperPy Team

# Categorization
plugin_type: llm
category: model_provider
provider_name: mock
entry_point: run_mesh.MockModelProvider

# Configuration schema
config_schema:
  type: object
  required: ["model_id"]
  properties:
    model_id:
      type: string
      description: Model identifier for the mock provider
      default: mock-model
    temperature:
      type: number
      description: Sampling temperature for response generation
      minimum: 0.0
      maximum: 2.0
      default: 0.7
    max_tokens:
      type: integer
      description: Maximum tokens to generate in response
      minimum: 1
      maximum: 4096
      default: 1000

# Default configuration
default_config:
  model_id: mock-model
  temperature: 0.7
  max_tokens: 1000

# Documentation
documentation:
  description: |
    Mock provider implementation for testing and development of AI Gateway functionality.
    This provider simulates LLM responses without requiring external API access.
    
  usage: |
    The mock provider can be used for:
    1. Testing AI Gateway integration
    2. Development without external API dependencies
    3. Validating gateway configuration
    4. Performance testing and benchmarking
    
  examples:
    - title: Basic Usage
      description: Simple chat request with mock provider
      code: |
        from pepperpy import PepperPy
        
        # Initialize with mock provider
        pepper = PepperPy().with_llm("mock")
        
        # Execute chat request
        response = await pepper.llm.chat("Test message")
        
    - title: Custom Configuration
      description: Configure mock provider behavior
      code: |
        from pepperpy import PepperPy
        
        # Initialize with custom config
        pepper = PepperPy().with_llm("mock", config={
            "model_id": "custom-mock",
            "temperature": 0.5,
            "max_tokens": 2000
        })
        
        # Execute request
        response = await pepper.llm.chat("Test message") 