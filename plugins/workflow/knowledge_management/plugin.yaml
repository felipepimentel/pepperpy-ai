name: workflow/knowledge_management
version: 0.1.0
author: PepperPy Team
description: Knowledge management and RAG workflows for document processing, embedding generation, and retrieval-augmented generation.
homepage: https://github.com/pepperpy/pepperpy
repository: https://github.com/pepperpy/pepperpy

config_schema:
  type: object
  properties:
    embedding_model:
      type: string
      description: Embedding model to use for vector embeddings
      default: text-embedding-ada-002
    vector_store:
      type: string
      description: Vector store backend to use
      enum: [chroma, faiss, qdrant]
      default: chroma
    chunk_size:
      type: integer
      description: Size of document chunks
      default: 1000
      minimum: 100
      maximum: 8000
    detail_level:
      type: string
      description: Level of detail in responses
      enum: [low, medium, high]
      default: medium
    auto_save_results:
      type: boolean
      description: Automatically save results to disk
      default: true
    response_format:
      type: string
      description: Format of response outputs
      enum: [text, json, markdown]
      default: text
  required:
    - vector_store

documentation:
  features:
    - Document chunking and preprocessing
    - Vector embedding generation
    - Knowledge base creation and management
    - RAG (Retrieval Augmented Generation)
    - Conversation memory and context management
  
  description: |
    This workflow plugin provides comprehensive knowledge management features:
    
    - **Document Processing**: Split documents into chunks for efficient processing
    - **Embedding Generation**: Create vector embeddings from text chunks
    - **Knowledge Base Management**: Store and retrieve documents with vector search
    - **RAG Capabilities**: Query knowledge bases to enhance responses with relevant information
    - **Conversation Memory**: Maintain conversation history for contextual responses

  usage_examples:
    - title: Create and Query a Knowledge Base
      language: python
      code: |
        from pepperpy import create_workflow
        
        # Create a knowledge management workflow
        workflow = create_workflow("knowledge_management", 
                                  vector_store="chroma",
                                  embedding_model="text-embedding-ada-002")
        
        # Sample documents
        documents = [
            "Retrieval Augmented Generation (RAG) is a technique that combines retrieval with generation...",
            "Vector databases store and retrieve embeddings efficiently for similarity search...",
            "Embeddings represent text as dense vectors in high-dimensional space..."
        ]
        
        # Create a knowledge base
        kb_id = await workflow.execute({
            "task": "create_kb",
            "input": {
                "name": "ai_concepts",
                "documents": documents,
                "description": "Knowledge base about AI concepts"
            }
        })["kb_id"]
        
        # Query the knowledge base
        result = await workflow.execute({
            "task": "query_kb",
            "input": {
                "kb_id": kb_id,
                "query": "How does RAG work?",
                "detail_level": "high"
            }
        })
        
        print(result["response"])

    - title: Use RAG with Conversation Memory
      language: python
      code: |
        from pepperpy import create_workflow
        
        # Create a knowledge management workflow
        workflow = create_workflow("knowledge_management")
        
        # Create a conversation with memory
        conversation_id = "user_123"
        
        # First interaction
        result = await workflow.execute({
            "task": "memory",
            "input": {
                "prompt": "Tell me about RAG",
                "conversation_id": conversation_id
            }
        })
        
        print(result["response"])
        
        # Follow-up question with RAG
        result = await workflow.execute({
            "task": "rag",
            "input": {
                "kb_id": "ai_concepts",
                "prompt": "How does it improve LLM responses?",
                "conversation_id": conversation_id,
                "history": result["history"]  # Pass previous history
            }
        })
        
        print(result["response"])

requirements:
  - chromadb>=0.4.6
  - sentence-transformers>=2.2.2
  - pydantic>=2.0.0 