name: workflow/llm_interaction
version: 0.1.0
description: Workflow for LLM interaction including chat, completion, streaming, and
  embedding
author: PepperPy Team
license: MIT
type: workflow
config_schema:
  provider:
    type: string
    description: LLM provider to use (e.g., openai, anthropic, llama)
    default: openai
  model:
    type: string
    description: LLM model to use
    default: gpt-3.5-turbo
  api_key:
    type: string
    description: API key for the LLM provider
  temperature:
    type: number
    description: Temperature for text generation (0.0 to 1.0)
    default: 0.7
    minimum: 0.0
    maximum: 1.0
  max_tokens:
    type: integer
    description: Maximum number of tokens to generate
    default: 1024
  streaming:
    type: boolean
    description: Whether to stream responses
    default: false
  system_prompt:
    type: string
    description: System prompt to use for chat
    default: You are a helpful assistant powered by the PepperPy framework.
  embedding_model:
    type: string
    description: Model to use for embeddings
    default: text-embedding-ada-002
  output_dir:
    type: string
    description: Directory to save results
    default: ./output/llm
documentation:
  description: 'LLM Interaction Workflow provides a comprehensive interface for interacting
    with

    Large Language Models, supporting:


    1. Text completion

    2. Chat conversations

    3. Streaming responses

    4. Text embeddings


    This workflow makes it easy to interact with various LLM providers through a

    consistent interface, with support for conversation history management and

    response streaming.

    '
  features:
  - Text Completion: Generate text completions with various models
  - Chat Interface: Create interactive chatbots with history management
  - Streaming: Stream responses for better UX with real-time output
  - Embeddings: Generate text embeddings for semantic search
  usage_examples:
  - title: Basic Text Completion
    python: "from pepperpy.workflow import create_provider\n\n# Create the LLM interaction\
      \ workflow provider\nworkflow = create_provider(\"llm_interaction\", \n    \
      \                      provider=\"openai\",\n                          model=\"\
      gpt-4\",\n                          api_key=\"your_api_key\")\n\n# Create input\
      \ for processing\ninput_data = {\n    \"task\": \"text_completion\",\n    \"\
      input\": {\n        \"prompt\": \"Explain what PepperPy is in one paragraph.\"\
      ,\n    }\n}\n\n# Execute workflow\nresult = await workflow.execute(input_data)\n\
      \n# Print the completion\nprint(result[\"text\"])\n"
  - title: Interactive Chat
    python: "# Create workflow with chat configuration\nworkflow = create_provider(\"\
      llm_interaction\", \n                          provider=\"openai\",\n      \
      \                    model=\"gpt-3.5-turbo\",\n                          api_key=\"\
      your_api_key\",\n                          system_prompt=\"You are a helpful\
      \ coding assistant.\")\n\n# Send multiple messages\nmessages = [\n    {\"role\"\
      : \"user\", \"content\": \"How can I use Python's asyncio library?\"}\n]\n\n\
      result = await workflow.execute({\n    \"task\": \"chat\",\n    \"input\": {\"\
      messages\": messages}\n})\n\n# Continue the conversation\nmessages.append({\"\
      role\": \"assistant\", \"content\": result[\"text\"]})\nmessages.append({\"\
      role\": \"user\", \"content\": \"Can you show me a simple example?\"})\n\nresult\
      \ = await workflow.execute({\n    \"task\": \"chat\",\n    \"input\": {\"messages\"\
      : messages}\n})\n\nprint(result[\"text\"])\n"
  - title: Streaming Chat with CLI
    shell: "# Run LLM interaction workflow via CLI with streaming enabled\npython\
      \ -m pepperpy.cli workflow run workflow/llm_interaction \\\n  --params \"provider=openai\"\
      \ \\\n  --params \"model=gpt-4\" \\\n  --params \"task=stream_chat\" \\\n  --params\
      \ \"prompt=Explain how LLMs work\" \\\n  --params \"streaming=true\"\n  \n"
  - title: Generate Embeddings
    python: "# Generate embeddings for semantic search\nworkflow = create_provider(\"\
      llm_interaction\", \n                          provider=\"openai\",\n      \
      \                    embedding_model=\"text-embedding-ada-002\",\n         \
      \                 api_key=\"your_api_key\")\n\n# Get embeddings for multiple\
      \ texts\nresult = await workflow.execute({\n    \"task\": \"embedding\",\n \
      \   \"input\": {\n        \"texts\": [\n            \"What is artificial intelligence?\"\
      ,\n            \"Explain machine learning algorithms.\",\n            \"How\
      \ do neural networks work?\"\n        ]\n    }\n})\n\n# Access the embeddings\n\
      embeddings = result[\"embeddings\"]\n"
requirements:
- pydantic>=2.0.0
- jsonschema>=4.0.0
- numpy>=1.24.0
plugin_type: plugins
provider_name: llm_interaction
entry_point: provider.Llm_interactionProvider
