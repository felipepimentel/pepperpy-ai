name: llm/local
version: 0.1.0
description: Local LLM provider for running models on your own hardware
author: PepperPy Team

plugin_type: llm
category: provider
provider_name: local
entry_point: provider.LocalProvider

config_schema:
  type: object
  properties:
    api_type:
      type: string
      description: API type to use (llama.cpp, oobabooga, openai-compatible)
      default: "llama.cpp"
    model_path:
      type: string
      description: Path to the local model file (for llama.cpp)
    host:
      type: string
      description: Host for the API server
      default: "localhost"
    port:
      type: integer
      description: Port for the API server
      default: 8000
    model:
      type: string
      description: Model name or path to use for API requests
      default: "default"
    context_length:
      type: integer
      description: Context length in tokens
      default: 2048
    threads:
      type: integer
      description: Number of threads to use for inference
    temperature:
      type: number
      description: Sampling temperature (0-1)
      default: 0.7
    max_tokens:
      type: integer
      description: Maximum number of tokens to generate
      default: 1024
    top_p:
      type: number
      description: Nucleus sampling parameter (0-1)
      default: 1.0

default_config:
  api_type: "llama.cpp"
  host: "localhost"
  port: 8000
  model: "default"
  context_length: 2048
  temperature: 0.7
  max_tokens: 1024
  top_p: 1.0

# Examples for testing the plugin
examples:
  - name: "basic_chat"
    description: "Basic chat functionality test"
    input:
      task: "chat"
      messages:
        - role: "user"
          content: "Hello, how are you?"
      options:
        temperature: 0.7
    expected_output:
      status: "success"
  
  - name: "text_completion"
    description: "Text completion test"
    input:
      task: "completion"
      prompt: "Once upon a time"
      options:
        max_tokens: 50
    expected_output:
      status: "success"
