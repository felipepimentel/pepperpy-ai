name: Local LLM Provider
version: 0.1.0
description: Provider for local language models using transformers
author: PepperPy Team
plugin_category: llm
provider_type: local
required_config_keys:
- model
default_config:
  model: llama2
  model_path: null
  device: cpu
  temperature: 0.7
  max_tokens: 100
config_schema:
  model:
    description: 'Model identifier (default: llama2)'
    required: true
    type: string
    default: llama2
  model_path:
    description: Optional path to model weights (defaults to model identifier if not
      specified)
    required: false
    type: string
  device:
    description: Device to run model on (cpu, cuda, mps)
    required: false
    default: cpu
    type: string
  temperature:
    description: Sampling temperature between 0.0 (deterministic) and 2.0 (creative)
    required: false
    default: 0.7
    type: float
    min: 0.0
    max: 2.0
  max_tokens:
    description: Maximum tokens to generate in the response
    required: false
    default: 100
    type: integer
    min: 1
documentation:
  usage: "Local LLM provider for running models on your own hardware.\n\nRequires\
    \ a model identifier or path and the appropriate hardware.\n\nExample usage:\n\
    ```python\nfrom pepperpy import create_provider\n\n# Use a local model\nprovider\
    \ = create_provider(\"llm\", \"local\", model=\"llama2\", device=\"cuda\")\n\n\
    # Or with a specific model path\nprovider = create_provider(\n    \"llm\", \n\
    \    \"local\", \n    model=\"mistral\", \n    model_path=\"/path/to/mistral/weights\"\
    ,\n    device=\"cuda\"\n)\n\nresponse = await provider.generate(\"Tell me a joke.\"\
    )\nprint(response.content)\n```\n"
  models:
  - llama2
  - mistral
  - phi
  - Any HuggingFace model ID or local path
category: llm
provider_name: local
entry_point: provider.LocalProvider
pepperpy_compatibility: '>=0.1.0'
